{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "\n",
    "This hands-on session is split into **2 parts**: a (very) short one with synthetic data to experiment a bit with SVM models, and a longer one aiming at predicting avalanches from wheater data (based on real data).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries on synthetic data\n",
    "\n",
    "The goal here is to experience the examples seen during the lecture, concerning the robustness to outliers.\n",
    "\n",
    "Below is a synthetic dataset, and you'll learn an SVM classifier and see what happens when having outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make 2-class dataset for classification\n",
    "centers = [[-10, 0], [10, 0]]\n",
    "x, y = make_blobs(n_samples=100, centers=centers, random_state=40)\n",
    "\n",
    "\n",
    "# Add some biasing points\n",
    "center_noise = [[10, 4], [-10, -4]]\n",
    "x_noise, y_noise = make_blobs(n_samples=3, centers=center_noise, random_state=4, cluster_std=0.1)\n",
    "xbiased = np.concatenate((x,x_noise))\n",
    "ybiased = np.concatenate((y,y_noise))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the dataset with a scatter plot, and plot the margins with various soft margin strenghts. The soft margins strengths in the ```SVC``` function are controlled with the parameter ```C```. Try values in the range ```[0,1]``` and report a value of strenght that looks like this figure :  ![title](svm.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function for plotting dataset, decision rule and margins of a SVM classifier\n",
    "def plot_svm(clf,x,y):\n",
    "    # Adapted from https://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(-20, 20)\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "    # plot the parallels to the separating hyperplane that pass through the\n",
    "    # support vectors (margin away from hyperplane in direction\n",
    "    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n",
    "    # 2-d.\n",
    "    margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n",
    "    yy_down = yy - np.sqrt(1 + a ** 2) * margin\n",
    "    yy_up = yy + np.sqrt(1 + a ** 2) * margin\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.figure(1, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "    plt.plot(xx, yy, \"k-\")\n",
    "    plt.plot(xx, yy_down, \"k--\")\n",
    "    plt.plot(xx, yy_up, \"k--\")\n",
    "    plt.scatter(\n",
    "        clf.support_vectors_[:, 0],\n",
    "        clf.support_vectors_[:, 1],\n",
    "        s=80,\n",
    "        facecolors=\"none\",\n",
    "        zorder=10,\n",
    "        edgecolors=\"k\",\n",
    "        cmap=plt.cm.get_cmap(\"RdBu\"),\n",
    "    )\n",
    "    plt.scatter(\n",
    "        xbiased[:, 0], xbiased[:, 1], c=ybiased*0.5, zorder=10, cmap=plt.cm.get_cmap(\"brg\"), edgecolors=\"k\"\n",
    "    )\n",
    "    plt.axis(\"tight\")\n",
    "    x_min = -20\n",
    "    x_max = 20\n",
    "    y_min = -10\n",
    "    y_max = 10\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "    \n",
    "    # Put the result into a contour plot\n",
    "    plt.contourf(XX, YY, Z, cmap=plt.cm.get_cmap(\"brg\"), alpha=0.5, linestyles=[\"-\"])\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "# fit the model\n",
    "penalties  =#TO COMPLETE (1 expression)\n",
    "for penalty in penalties:\n",
    "    clf = svm.SVC(kernel=\"linear\",#TO COMPLETE (1 expression)\n",
    "    clf.fit(xbiased, ybiased)\n",
    "    plot_svm(clf,xbiased,ybiased)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the benefit of setting this penalty when fitting the model? How do you quantify the robustness of the decision rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To what model parameter the following figure corresponds to:  ![title](svm0.png) \n",
    "    \n",
    "What is the name of this model?\n",
    "#TO COMPLETE\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avalanche dataset\n",
    "\n",
    "We now focus on real data. The goal is is to predict an avalanche from weather data. The dataset is a real dataset which has been built by Ensimag students in 2019, through a \"Projet de spécialité\". Thanks to their work, we can now enjoy predicting avalanches for free.\n",
    "\n",
    "They scrapped data from different sources, and only retain here the part of the data related to cross-country ski-related avalanches.\n",
    "\n",
    "We will use the following descriptors (names should be explicit enough, given that dens menans density and  rad, radiation):\n",
    "\n",
    "`\"6am_temp\",\"16pm_temp\",\"precip\",\"snow_fall\",\"snow_dens\",\"snow_depth\",\"solar_rad\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"avalanche_data.csv\", header = 0)\n",
    "\n",
    "# Balance the labels (as many 1 and 0) -> we drop many non-avalanche events\n",
    "balanced_data = data[0:2448].sample(frac=1).to_numpy()\n",
    "\n",
    "header = [\"id\",\"avalanche\",\"data_source\",\"orientation\",\"date\",\"long\",\"lat\",\"6am_temp\",\"16pm_temp\",\"precip\",\"snow_fall\",\"snow_dens\",\"snow_depth\",\"solar_rad\"\n",
    "]\n",
    "\n",
    "# Data descriptors\n",
    "raw_data = balanced_data[:, 7:14].astype(float)\n",
    "# normalize the data using preprocessing.StandardScaler() call\n",
    "x =#TO COMPLETE (1 expression)\n",
    "kept_headers = header[7:14]\n",
    "y = balanced_data[:,1].astype(int) # y=1 means avalanche, y=0 means no avalanche\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data: make a PCA, colored by class (avalanche or not). Is there a chance that we can classify our data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca =#TO COMPLETE (1 expression)\n",
    "lower_dim_data =#TO COMPLETE (1 expression)\n",
    "plt.clf()\n",
    "plt.scatter(lower_dim_data[:,0],lower_dim_data[:,1],c=y) # color is y\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment on the structure of the data: will it be easy to predict avalanches or not? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "### SVM\n",
    "Learn a simple classifier: an SVM for instance. Show some useful statistics on the performances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "svm_avalanche =#TO COMPLETE (1 expression)\n",
    "\n",
    "# Split data into 50% train and 50% test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.5, shuffle=True\n",
    ")\n",
    "\n",
    "# Learn the digits on the train subset\n",
    "#TO COMPLETE\n",
    "\n",
    "# Predict the value of the digit on the test subset\n",
    "predicted_svm =#TO COMPLETE (1 expression)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can optimize your classifier by changing the kernel, the `gamma` parameter of the RBF (Gaussian) kernel, as well as the soft margins. Find optimal parameters that optimize the performances on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretable models\n",
    "\n",
    "We want to get a bit of insights of why there are avalanches, so we are going to use decision trees/random forest.\n",
    "\n",
    "#### Decision tree\n",
    "\n",
    "Make a [decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) with few leaves (e.g. 3) so that we can interpret easily the decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_avalanche = DecisionTreeClassifier(#TO COMPLETE (1 expression)\n",
    "tree_avalanche.fit(X_train, y_train)\n",
    "predicted_tree =#TO COMPLETE (1 expression)\n",
    "accuracy_tree_test =#TO COMPLETE (1 expression)\n",
    "print(f\"Accuracy on test set : { accuracy_tree_test}\")\n",
    "\n",
    "predicted_train_tree = tree_avalanche.predict(X_train)\n",
    "accuracy_tree_train =#TO COMPLETE (1 expression)\n",
    "print(f\"Accuracy on train set : { accuracy_tree_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good thing with decision tree is that we can visualize the decisions. Use the command `tree.plot_tree`, setting the flag `filled=True` for more readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the decision in terms of what it means physically, by matching the decision indicies with the headers (contained in the var `kept_headers`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to increase the accuracy, we will have to make more splits... which is an issue as pointed out in the lectures. Verify this fact by yourself by monitoring the difference of accuracy between the train and validation set. When does it overfit?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = list()\n",
    "accuracy_test = list()\n",
    "\n",
    "leaves =#TO COMPLETE (1 expression)\n",
    "\n",
    "for nb_leaf in leaves:\n",
    "    tree_avalanche = DecisionTreeClassifier(max_leaf_nodes=nb_leaf,#TO COMPLETE (1 expression)\n",
    "#TO COMPLETE\n",
    "    predicted_tree_validation =#TO COMPLETE (1 expression)\n",
    "    predicted_tree_train =#TO COMPLETE (1 expression)\n",
    "    accuracy_test +=#TO COMPLETE (1 expression)\n",
    "    accuracy_train +=#TO COMPLETE (1 expression)\n",
    "    \n",
    "    \n",
    "\n",
    "plt.clf()\n",
    "plt.plot(leaves,accuracy_train,leaves,accuracy_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest\n",
    "\n",
    "We switch to [random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to see if we can get better accuracy without overfitting. Give a reason why RF are less prone to overfitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy_train = list()\n",
    "accuracy_test = list()\n",
    "\n",
    "leaves =#TO COMPLETE (1 expression)\n",
    "\n",
    "for nb_leaf in leaves:\n",
    "    rf_avalanche = RandomForestClassifier(max_leaf_nodes=nb_leaf,#TO COMPLETE (1 expression)\n",
    "#TO COMPLETE\n",
    "    predicted_rf_validation =#TO COMPLETE (1 expression)\n",
    "    predicted_rf_train =#TO COMPLETE (1 expression)\n",
    "    accuracy_test +=#TO COMPLETE (1 expression)\n",
    "    accuracy_train +=#TO COMPLETE (1 expression)\n",
    "    \n",
    "    \n",
    "\n",
    "plt.clf()\n",
    "plt.plot(leaves,accuracy_train,leaves,accuracy_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on your results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot anymore visualize the tree, but we can **check the feature importance**. Take a look at the attribute `feature_importances_` of your random forest. What is the meaning of these numbers? Which feature look the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
